{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Transformer Example \n",
    "\n",
    "Without using a real dataset - for learning purposes\n",
    "\n",
    "Materials: http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "omitted causal masking and a few dropout layers for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = 10\n",
    "embedding_dim = 4\n",
    "scale_factor = 1.0 / np.sqrt(embedding_dim)\n",
    "max_seq_length = 4\n",
    "num_heads = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = torch.nn.Embedding(dict_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1, 2, 0, 2], [0, 4, 2, 9]])\n",
    "embedding = embedding_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.LongTensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder():\n",
    "    def __init__(self, embedding_dim, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = torch.zeros((1, max_seq_length, embedding_dim), requires_grad=False)\n",
    "        \n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(0, embedding_dim, 2):\n",
    "                self.positional_encoding[0, pos, i] = np.sin(float(pos) / (10000 ** (2 * float(i)/embedding_dim)))\n",
    "                self.positional_encoding[0, pos, i + 1] = np.cos(float(pos) / (10000 ** (2 * float(i)/embedding_dim)))        \n",
    "    \n",
    "    def get(self):\n",
    "        return self.positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query, keys, values using the embeddings\n",
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, scale_factor):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.w_q = torch.nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = torch.nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = torch.nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def compute_attention(self, q, k, v, input, embedding):\n",
    "        mask = np.zeros(embedding.shape)\n",
    "        mask[input > 0.] = 1.0\n",
    "        \n",
    "        attention = torch.nn.functional.softmax(torch.bmm(q, torch.transpose(k, 2, 1)) * self.scale_factor, dim = -1)\n",
    "        attention = torch.masked_fill(attention, torch.Tensor(mask) == 0, -1e9)\n",
    "        attention = torch.bmm(attention, v)\n",
    "        \n",
    "        return attention\n",
    "        \n",
    "    def forward(self, input, embedding):\n",
    "        q = self.w_q(embedding)\n",
    "        k = self.w_k(embedding)\n",
    "        v = self.w_v(embedding)\n",
    "        \n",
    "        return self.compute_attention(q, k, v, input, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multi-head attention\n",
    "class MultiAttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length, num_heads, scale_factor):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.heads = torch.nn.ModuleList([AttentionHead(embedding_dim, scale_factor) for _ in range(num_heads)])\n",
    "        self.linear = torch.nn.Linear(num_heads * embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.norm_layer = torch.nn.LayerNorm([max_seq_length, embedding_dim])\n",
    "    \n",
    "    def forward(self, input, embedding):\n",
    "        heads_attention = torch.cat([head(input, embedding) for head in self.heads], dim=-1)\n",
    "\n",
    "        heads_attention = self.linear(heads_attention)\n",
    "        \n",
    "        # normalize\n",
    "        heads_attention = self.norm_layer(heads_attention)\n",
    "        \n",
    "        return heads_attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, dim_net=64):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(embedding_dim, dim_net)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.linear2 = torch.nn.Linear(dim_net, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length, num_heads, scale_factor):\n",
    "        super().__init__()\n",
    "        self.attention_head = MultiAttentionHead(embedding_dim, max_seq_length, num_heads, scale_factor)\n",
    "        self.ffw_net = FeedForward(embedding_dim)\n",
    "        self.encoder_norm = torch.nn.LayerNorm([max_seq_length, embedding_dim])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.attention_head(input, x)\n",
    "        x = self.ffw_net(x)\n",
    "        # + residual\n",
    "        x += x\n",
    "        x = self.encoder_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length, num_heads, scale_factor, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = torch.nn.ModuleList([EncoderLayer(embedding_dim, max_seq_length, num_heads, scale_factor) for _ in range(num_layers)])\n",
    "        self.pe = PositionalEncoder(embedding_dim, max_seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x += self.pe.get()\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length, num_heads, scale_factor):\n",
    "        super().__init__()\n",
    "        self.init_norm = torch.nn.LayerNorm([max_seq_length, embedding_dim])\n",
    "        self.attention_head1 = MultiAttentionHead(embedding_dim, max_seq_length, num_heads, scale_factor)\n",
    "        self.attention_head2 = MultiAttentionHead(embedding_dim, max_seq_length, num_heads, scale_factor)\n",
    "        self.ffw_net = FeedForward(embedding_dim)\n",
    "        self.decoder_norm1 = torch.nn.LayerNorm([max_seq_length, embedding_dim])\n",
    "        self.decoder_norm2 = torch.nn.LayerNorm([max_seq_length, embedding_dim])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.init_norm(x)\n",
    "        \n",
    "        x = self.attention_head1(input, x)\n",
    "        x = self.ffw_net(x)\n",
    "        # + residual\n",
    "        x += x\n",
    "        x = self.decoder_norm1(x)\n",
    "        x = self.attention_head2(input, x)\n",
    "        x = self.ffw_net(x)\n",
    "        x += x\n",
    "        x = self.decoder_norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length, num_heads, scale_factor, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.layers = torch.nn.ModuleList([EncoderLayer(embedding_dim, max_seq_length, num_heads, scale_factor) for _ in range(num_layers)])\n",
    "        self.pe = PositionalEncoder(embedding_dim, max_seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x)\n",
    "        x += self.pe.get()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length, \n",
    "                 num_heads, scale_factor, \n",
    "                 num_layers_encoder=2, num_layers_decoder=2,\n",
    "                 target_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(embedding_dim, max_seq_length, num_heads, scale_factor, num_layers_encoder)\n",
    "        self.decoder = Decoder(embedding_dim, max_seq_length, num_heads, scale_factor, num_layers_decoder)\n",
    "        self.out = torch.nn.Linear(embedding_dim * max_seq_length, target_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_x = self.encoder(x)\n",
    "        decoder_x = self.decoder(encoder_x)\n",
    "        decoder_x = torch.reshape(decoder_x, (decoder_x.shape[0], -1))\n",
    "        out = self.out(decoder_x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(embedding_dim, max_seq_length, num_heads, scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3799, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3296, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3300, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2916, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5611, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4924, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5628, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3984, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4316, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2967, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2839, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2568, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4196, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6779, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2094, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2250, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2015, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3940, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2442, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2876, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2779, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2598, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4255, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2955, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3950, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2279, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2644, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2542, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3072, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3069, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2446, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4938, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2208, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2358, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3186, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3026, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2853, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1981, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2627, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1972, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2845, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1923, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2269, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2682, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2736, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1807, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1797, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3853, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1961, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2599, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2146, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2675, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2317, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1954, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2403, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1428, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2544, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1938, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1994, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1910, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2286, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1473, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1527, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1817, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2370, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1574, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2196, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2527, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1848, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2259, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2203, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1568, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2409, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1991, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1425, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1605, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2175, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2330, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1504, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2969, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1830, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1724, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3552, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1629, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2783, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1989, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1790, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1807, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1951, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2814, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2642, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1833, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1418, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1492, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1632, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4058, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1577, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2056, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1649, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1717, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1474, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1327, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1532, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1487, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1331, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1573, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1589, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3417, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1475, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1992, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1469, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3623, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1416, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1792, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1359, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1488, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1695, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2019, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1322, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1596, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1453, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1510, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1155, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1826, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1446, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1086, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1458, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1563, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1344, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1613, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1624, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1438, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1315, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1083, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1361, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1447, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1544, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1460, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1318, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1318, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1226, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1334, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1235, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1747, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1280, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1261, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1200, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1645, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1080, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1521, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1237, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1663, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1590, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1233, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1351, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1543, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1333, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1473, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1284, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1196, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1305, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1419, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1334, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1347, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1291, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0846, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1205, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1506, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1162, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1405, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1444, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1343, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1507, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1616, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1594, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1177, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1170, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1276, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1344, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1368, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1099, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1292, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1166, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1475, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1361, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1358, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1120, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1238, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1176, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1421, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1728, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1485, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1218, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1349, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1345, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1269, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1173, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1090, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1171, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1140, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1179, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1183, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1439, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1355, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1054, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1194, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1048, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1255, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1079, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1253, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1212, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1272, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1043, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1156, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1075, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1728, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0969, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1310, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1079, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1185, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1166, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1019, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1114, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0845, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0935, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1042, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2602, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0960, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1091, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1031, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1265, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0958, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0915, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1207, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1043, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0974, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0848, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1414, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0856, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0885, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1456, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0811, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1214, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0984, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1199, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0908, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0960, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0929, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1416, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0919, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0956, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0989, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0941, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0885, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0933, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0919, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0913, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0967, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0933, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0905, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0997, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0891, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0941, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0938, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0838, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1244, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0945, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0934, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1034, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0753, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0774, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0793, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0875, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1083, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0933, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0782, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1056, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0939, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0853, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0897, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0948, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0845, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0875, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0821, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0904, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1076, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0923, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0792, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0923, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1079, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0861, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0882, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0890, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0984, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1015, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1046, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0933, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0894, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0860, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1057, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0804, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0685, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0834, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0981, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0616, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0793, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0999, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0842, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0649, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0789, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0925, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0940, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0595, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0649, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0621, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0558, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0515, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0457, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0731, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0595, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0438, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0335, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0361, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0336, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    output = model(embedding)\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss_value = loss(output, target)\n",
    "    print(loss_value)\n",
    "    \n",
    "    loss_value.backward(retain_graph=True)\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
